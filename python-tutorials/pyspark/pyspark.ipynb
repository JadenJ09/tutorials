{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/03 18:25:18 WARN Utils: Your hostname, razer-demian resolves to a loopback address: 127.0.1.1; using 192.168.178.51 instead (on interface enxa0cec8e9c005)\n",
      "24/03/03 18:25:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/03 18:25:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default setting\n",
    "rdd_par = spark.sparkContext.parallelize(dataset_name, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with partition argument of 10\n",
    "# Spark’s default is to partition the text file in 128 MB blocks\n",
    "rdd_txt = spark.sparkContext.textFile(\"file_name.txt\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_txt.getNumPartitions()\n",
    "# output: 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To end the Spark session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "# Transformations\n",
    "add_one = lambda x: x+1 # apply x+1 to x\n",
    "print(add_one(10)) # this will output 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "student_data = [(\"Chris\",1523,0.72,\"CA\"),\n",
    "                (\"Jake\", 1555,0.83,\"NY\"),\n",
    "                (\"Cody\", 1439,0.92,\"CA\"),\n",
    "                (\"Lisa\",1442,0.81,\"FL\"),\n",
    "                (\"Daniel\",1600,0.88,\"TX\"),\n",
    "                (\"Kelvin\",1382,0.99,\"FL\"),\n",
    "                (\"Nancy\",1442,0.74,\"TX\"),\n",
    "                (\"Pavel\",1599,0.82,\"NY\"),\n",
    "                (\"Josh\",1482,0.78,\"CA\"),\n",
    "                (\"Cynthia\",1582,0.94,\"CA\")]\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "student_rdd = spark.sparkContext.parallelize(student_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72.0, 'CA'),\n",
       " ('Jake', 1555, 83.0, 'NY'),\n",
       " ('Cody', 1439, 92.0, 'CA'),\n",
       " ('Lisa', 1442, 81.0, 'FL'),\n",
       " ('Daniel', 1600, 88.0, 'TX'),\n",
       " ('Kelvin', 1382, 99.0, 'FL'),\n",
       " ('Nancy', 1442, 74.0, 'TX'),\n",
       " ('Pavel', 1599, 82.0, 'NY'),\n",
       " ('Josh', 1482, 78.0, 'CA'),\n",
       " ('Cynthia', 1582, 94.0, 'CA')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map transformation\n",
    "rdd_transformation = student_rdd.map(lambda x: (x[0], x[1], x[2] * 100, x[3]))\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_transformation.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jake', 1555, 83.0, 'NY'),\n",
       " ('Cody', 1439, 92.0, 'CA'),\n",
       " ('Lisa', 1442, 81.0, 'FL'),\n",
       " ('Daniel', 1600, 88.0, 'TX'),\n",
       " ('Kelvin', 1382, 99.0, 'FL'),\n",
       " ('Pavel', 1599, 82.0, 'NY'),\n",
       " ('Cynthia', 1582, 94.0, 'CA')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter transformation\n",
    "rdd_filtered = rdd_transformation.filter(lambda x: x[2] > 80)\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_filtered.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72.0, 'CA'),\n",
       " ('Jake', 1555, 83.0, 'NY'),\n",
       " ('Cody', 1439, 92.0, 'CA'),\n",
       " ('Lisa', 1442, 81.0, 'FL'),\n",
       " ('Daniel', 1600, 88.0, 'TX')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_transformation.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "843.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reduce transformation\n",
    "sum_gpa = rdd_transformation.map(lambda x: x[2]).reduce(lambda x,y: x+y)\n",
    "\n",
    "# view the sum\n",
    "sum_gpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84.3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_gpa / rdd_transformation.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Associative and Commutative Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark1 = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition:  [[1, 2, 3, 4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1, 2], [3, 4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1], [2, 3], [4, 5]]\n",
      "addition:  15\n",
      "partition:  [[1], [2], [3], [4, 5]]\n",
      "addition:  15\n"
     ]
    }
   ],
   "source": [
    "data = [1,2,3,4,5]\n",
    "for i in range(1,5):\n",
    "    rdd = spark1.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('addition: ', rdd.reduce(lambda a,b: a+b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "partition:  [[1, 2, 3, 4, 5]]\n",
      "division:  0.008333333333333333\n",
      "partition:  [[1, 2], [3, 4, 5]]\n",
      "division:  3.3333333333333335\n",
      "partition:  [[1], [2, 3], [4, 5]]\n",
      "division:  1.875\n",
      "partition:  [[1], [2], [3], [4, 5]]\n",
      "division:  0.20833333333333331\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,5):\n",
    "    rdd = spark1.sparkContext.parallelize(data, i)\n",
    "    print('partition: ', rdd.glom().collect())\n",
    "    print('division: ', rdd.reduce(lambda a,b: a/b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Broadcast Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of states\n",
    "states = ['FL', 'NY', 'TX', 'CA', 'NY', 'NY', 'FL', 'TX']\n",
    "# convert to RDD\n",
    "states_rdd = spark.sparkContext.parallelize(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of regions\n",
    "region = {\"NY\":\"East\", \"CA\":\"West\", \"TX\":\"South\", \"FL\":\"South\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['South', 'East', 'South', 'West']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcast region dictionary to nodes\n",
    "broadcast_var = spark.sparkContext.broadcast(region)\n",
    "# map regions to states\n",
    "result = states_rdd.map(lambda x: broadcast_var.value[x])\n",
    "# view first four results\n",
    "result.take(4)\n",
    "# output : [‘South’, ‘East’, ‘South’, ‘West’]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = {\"NY\":\"New York\", \"CA\":\"California\", \"TX\":\"Texas\", \"FL\":\"Florida\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.broadcast.Broadcast"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastStates = spark.sparkContext.broadcast(states)\n",
    "\n",
    "# confirm type\n",
    "type(broadcastStates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 1523, 72.0, 'California'),\n",
       " ('Jake', 1555, 83.0, 'New York'),\n",
       " ('Cody', 1439, 92.0, 'California'),\n",
       " ('Lisa', 1442, 81.0, 'Florida'),\n",
       " ('Daniel', 1600, 88.0, 'Texas'),\n",
       " ('Kelvin', 1382, 99.0, 'Florida'),\n",
       " ('Nancy', 1442, 74.0, 'Texas'),\n",
       " ('Pavel', 1599, 82.0, 'New York'),\n",
       " ('Josh', 1482, 78.0, 'California'),\n",
       " ('Cynthia', 1582, 94.0, 'California')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_broadcast = rdd_transformation.map(lambda x: (x[0],x[1],x[2], broadcastStates.value[x[3]]))\n",
    "\n",
    "# confirm transformation is correct\n",
    "rdd_broadcast.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accumulator Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.accumulators.Accumulator"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sat_1500 = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# confirm type\n",
    "type(sat_1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function count_high_sat_score at 0x7f2371e885e0>\n"
     ]
    }
   ],
   "source": [
    "def count_high_sat_score(r):\n",
    "    if r[1] > 1500: sat_1500.add(1)\n",
    "\n",
    "# confirm saved as a function\n",
    "print(count_high_sat_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "rdd_broadcast.foreach(lambda x: count_high_sat_score(x))\n",
    "\n",
    "# confirm accumulator worked\n",
    "print(sat_1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PySpark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/04 11:20:48 WARN Utils: Your hostname, razer-demian resolves to a loopback address: 127.0.1.1; using 192.168.178.51 instead (on interface enxa0cec8e9c005)\n",
      "24/03/04 11:20:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/04 11:20:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "sample_page_views  = spark.sparkContext.parallelize([\n",
    "    [\"en\", \"Statue_of_Liberty\", \"2022-01-01\", 263],\n",
    "    [\"en\", \"Replicas_of_the_Statue_of_Liberty\", \"2022-01-01\", 11],\n",
    "    [\"en\", \"Statue_of_Lucille_Ball\" ,\"2022-01-01\", 6],\n",
    "    [\"en\", \"Statue_of_Liberty_National_Monument\", \"2022-01-01\", 4],\n",
    "    [\"en\", \"Statue_of_Liberty_play\"  ,\"2022-01-01\", 3],  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------------------------+----------+-----+\n",
      "|language_code|title                              |date      |count|\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "|en           |Statue_of_Liberty                  |2022-01-01|263  |\n",
      "|en           |Replicas_of_the_Statue_of_Liberty  |2022-01-01|11   |\n",
      "|en           |Statue_of_Lucille_Ball             |2022-01-01|6    |\n",
      "|en           |Statue_of_Liberty_National_Monument|2022-01-01|4    |\n",
      "|en           |Statue_of_Liberty_play             |2022-01-01|3    |\n",
      "+-------------+-----------------------------------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD to DataFrame\n",
    "sample_page_views_df = sample_page_views.toDF(\n",
    "    ['language_code', 'title', 'date', 'count']\n",
    ")\n",
    "\n",
    "# show first 5 rows\n",
    "sample_page_views_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sample_page_views_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(language_code='en', title='Statue_of_Liberty', date='2022-01-01', count=263),\n",
       " Row(language_code='en', title='Replicas_of_the_Statue_of_Liberty', date='2022-01-01', count=11),\n",
       " Row(language_code='en', title='Statue_of_Lucille_Ball', date='2022-01-01', count=6),\n",
       " Row(language_code='en', title='Statue_of_Liberty_National_Monument', date='2022-01-01', count=4),\n",
       " Row(language_code='en', title='Statue_of_Liberty_play', date='2022-01-01', count=3)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DataFrame to RDD\n",
    "sample_page_views_rdd_restored = sample_page_views_df.rdd\n",
    "\n",
    "# show restored RDD\n",
    "sample_page_views_rdd_restored.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sample_page_views_rdd_restored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark DataFrames from External Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from a CSV file on disk\n",
    "wiki_uniq_w_schema_df = spark\\\n",
    "    .read\\\n",
    "    .option('header', True)\\\n",
    "    .option('delimiter', ',')\\\n",
    "    .option('inferSchema', True)\\\n",
    "    .csv('wiki_uniq_march_2022.csv')\n",
    "\n",
    "# show the data types\n",
    "wiki_uniq_w_schema_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting and Cleaning Data With PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# read in the Wikipedia unique visitors dataset\n",
    "uniq_views_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the schema\n",
    "uniq_views_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the data frame\n",
    "uniq_views_df_desc = uniq_views_df.describe()\n",
    "\n",
    "# show summary\n",
    "uniq_views_df_desc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "uniq_counts_human_df = uniq_views_df.drop('total_visitor_count','uniq_bot_visitors')\n",
    "\n",
    "# show the first 5 rows\n",
    "uniq_counts_human_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename column\n",
    "uniq_counts_final_df = uniq_counts_human_df.withColumnRenamed('uniq_human_visitors', 'unique_site_visitors')\n",
    "\n",
    "# show the first 5 rows\n",
    "uniq_counts_final_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a New SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in Wikipedia Unique Visitors Dataset\n",
    "wiki_uniq_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for Arabic site visitors\n",
    "ar_site_visitors = wiki_uniq_df\\\n",
    "    .filter(wiki_uniq_df.language_code == 'ar')\n",
    "\n",
    "# show the DataFrame\n",
    "ar_site_visitors.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select columns\n",
    "ar_visitors_slim = wiki_uniq_df\\\n",
    "    .select(['domain', 'uniq_human_visitors'])\\\n",
    "    .filter(wiki_uniq_df.language_code == 'ar')\n",
    "\n",
    "# show the DataFrame\n",
    "ar_visitors_slim.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by site type and sum unique visitors\n",
    "top_visitors_site_type = wiki_uniq_df.select(['site_type', 'uniq_human_visitors'])\\\n",
    "    .groupBy('site_type')\\\n",
    "    .sum()\\\n",
    "    .orderBy('sum(uniq_human_visitors)', ascending=False)\n",
    "\n",
    "# show the DataFrame\n",
    "top_visitors_site_type.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying PySpark with SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"learning_spark_sql\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in Wikipedia Unique Visitors Dataset\n",
    "wiki_uniq_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022_w_site_type.csv\")\n",
    "\n",
    "# Create a temporary view with the DataFrame\n",
    "# This will allow us to use SQL to query the data\n",
    "wiki_uniq_df\\\n",
    "    .createOrReplaceTempView('uniq_visitors_march')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to select Arabic site visitors\n",
    "ar_site_visitors_qry = \"\"\"\n",
    "    SELECT * FROM uniq_visitors_march\n",
    "    WHERE language_code = 'ar';\n",
    "\"\"\"\n",
    "\n",
    "# show the DataFrame\n",
    "spark\\\n",
    "    .sql(ar_site_visitors_qry)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to select specific columns and filter for Arabic site visitors\n",
    "ar_site_visitors_slim_qry = \"\"\"\n",
    "    SELECT domain, uniq_human_visitors \n",
    "    FROM uniq_visitors_march\n",
    "    WHERE language_code = 'ar';\n",
    "\"\"\"\n",
    "\n",
    "# show the DataFrame\n",
    "spark\\\n",
    "    .sql(ar_site_visitors_slim_qry)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query to group by site type and sum unique visitors\n",
    "site_top_type_qry = \"\"\"\n",
    "    SELECT site_type, SUM(uniq_human_visitors) \n",
    "    FROM uniq_visitors_march\n",
    "    GROUP BY site_type\n",
    "    ORDER BY SUM(uniq_human_visitors) DESC;\n",
    "\"\"\"\n",
    "\n",
    "# show the DataFrame\n",
    "spark\\\n",
    "    .sql(site_top_type_qry)\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Saving PySpark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a new SparkSession\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config('spark.app.name', 'learning_spark_sql')\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read in Wikipedia Unique Visitors Dataset\n",
    "wiki_uniq_df = spark.read\\\n",
    "    .option('header', True) \\\n",
    "    .option('delimiter', ',') \\\n",
    "    .option('inferSchema', True) \\\n",
    "    .csv(\"wiki_uniq_march_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the domain and unique visitors\n",
    "uniq_human_visitors_df = wiki_uniq_df\\\n",
    "    .select('domain', 'uniq_human_visitors')\n",
    "\n",
    "# show the new DataFrame\n",
    "uniq_human_visitors_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the DataFrame to a CSV file\n",
    "uniq_human_visitors_df\\\n",
    "    .write.csv('./results/csv/uniq_human_visitors/',  mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the CSV file and show the schema\n",
    "spark.read.csv('./results/csv/uniq_human_visitors/').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the DataFrame to a Parquet file\n",
    "uniq_human_visitors_df\\\n",
    "    .write.parquet('./results/pq/uniq_human_visitors/', mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the Parquet file and show the schema\n",
    "# Parquet is a columnar storage format\n",
    "# Compared to CSV, Parquet is more efficient for reading and writing and inferSchema is not needed\n",
    "spark.read.parquet('./results/pq/uniq_human_visitors/').printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
